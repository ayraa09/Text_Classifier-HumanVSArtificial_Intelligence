# -*- coding: utf-8 -*-
"""Human VS AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8kekViFcxoezd00pBUwri0OxAt5wZUJ

#                 **TEXT CLASSIFIER: Human VS Artificial Intelligence**

In recent years, artificial intelligence has advanced significantly, leading to the widespread use of AI-generated text in various domains, including *journalism, research*, and *creative writing*. While AI-generated text can be highly sophisticated, distinguishing between human-written and machine-generated content remains **a crucial challenge**.



In recent years, artificial intelligence has advanced significantly, leading to the widespread use of AI-generated text in various domains, including journalism, research, and creative writing. While AI-generated text can be highly sophisticated, distinguishing between human-written and machine-generated content remains a crucial challenge.



Through this analysis, we gain insights into how AI-generated text differs in **structure, vocabulary,** and **readability**.



This project can have practical applications in academic integrity, misinformation detection, and AI content validation.
"""

#Core libraries
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

#Text processing and nlp
import nltk
from wordcloud import WordCloud
from collections import Counter
from nltk.corpus import stopwords
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download("stopwords")

#ML libraries
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

#Setting the style
sns.set_style("darkgrid")
plt.style.use("dark_background")

"""## **Loading the Dataset from Hugging Face**"""

pip install datasets

from datasets import load_dataset

"""token: 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"""

from huggingface_hub import login
login()

ds= load_dataset("NicolaiSivesind/human-vs-machine", "research_abstracts_labeled")

"""**Converting the Dataset to Pandas DataFrame**

We have Training, Validation and Test sets in our original dataset.

We shall use the Training and Testing sets for this project.
"""

df_train= pd.DataFrame(ds["train"])
df_test= pd.DataFrame(ds["test"])
df_train

df_test

"""We have loaded and also displayed the datasets with splits, one for training set and one for the test set."""

print(df_train.shape)
print(df_test.shape)

"""## **Exploratory Data Analysis (EDA)**

Checking the overall information structure of the split sets including null and non null values
"""

df_train.info()

df_test.info()

"""Using the describe function to view the statistical summary of the both split sets"""

df_train.describe()

df_test.describe()

"""### **Distribution of Labels using the countplot in sns**"""

fig,axes = plt.subplots(1, 2, figsize=(11,5))

#Plot for Train set
sns.countplot(data=df_train, x='label', palette='Blues', ax=axes[0])
axes[0].set_title("Train Set: Human vs Machine Distribution")
axes[0].set_xlabel("Labels (0: Human, 1: Machine)")
axes[0].set_ylabel("Count")

#Plot for Test set
sns.countplot(data=df_test, x='label', palette='Blues', ax=axes[1])
axes[1].set_title("Test Set: Human vs Machine Distribution")
axes[1].set_xlabel("Labels (0: Human, 1: Machine)")
axes[1].set_ylabel("Count")

plt.tight_layout()
plt.show()

"""**Joining texts for WordCloud**"""

text_data_train = " ".join(df_train["text"])
text_data_test = " ".join(df_test["text"])

wordcloud_train = WordCloud(width=800, height=400, background_color="black").generate(text_data_train)
wordcloud_test = WordCloud(width=800, height=400, background_color="black").generate(text_data_test)

"""A word cloud (also known as a tag cloud or text cloud) is a visual representation of text data, where words are displayed in varying sizes based on their frequency or importance in a given text. The more frequently a word appears in the text, the larger and bolder it appears in the word cloud. Less frequent words are smaller and less prominent."""

fig, axes = plt.subplots(1,2, figsize=(14,7))

#WordCloud for Train Set
axes[0].imshow(wordcloud_train, interpolation="bilinear")
axes[0].axis("off")
axes[0].set_title("Word Cloud - Train Set", fontsize=14)

#WordCloud for Test Set
axes[1].imshow(wordcloud_test, interpolation="bilinear")
axes[1].axis("off")
axes[1].set_title("Word Cloud - Test Set", fontsize=14)

"""similar words like 'study', 'model', 'system' show up frequently in both sets, 'model' being the highest in both the sets.
- We can take this into consideration while testing the model on unknown data.

### **Text length distribution with Histogram**
"""

df_train["text_length"] = df_train["text"].apply(len)
df_test["text_length"] = df_test["text"].apply(len)

fig, axes = plt.subplots(1,2, figsize=(14,5))

#Histogram for Train Set
sns.histplot(df_train["text_length"], bins=30, kde=True, color="crimson", ax=axes[0])
axes[0].set_title("Text Length Distribution - Train Set")
axes[0].set_xlabel("Text Length")
axes[0].set_ylabel("Frequency")

#Histogram for Test Set
sns.histplot(df_test["text_length"], bins=30, kde=True, color="crimson", ax=axes[1])
axes[1].set_title("Text Length Distribution - Test Set")
axes[1].set_xlabel("Text Length")
axes[1].set_ylabel("Frequency")

plt.tight_layout()
plt.show()

"""### **Average word count per label using BoxPlot**"""

df_train["word_count"] = df_train["text"].apply(lambda x: len(x.split()))
df_test["word_count"] = df_test["text"].apply(lambda x: len(x.split()))

fig, axes = plt.subplots(1,2, figsize=(11,7))

#Boxplot Train Set
sns.boxplot(x="label", y="word_count", data=df_train, palette="Blues", ax=axes[0])
axes[0].set_title("Word Count Distribution - Train Set")
axes[0].set_xlabel("Label (Human vs. AI)")
axes[0].set_ylabel("Word Count")

#Boxplot Test Set
sns.boxplot(x="label", y="word_count", data=df_test, palette="Blues", ax=axes[1])
axes[1].set_title("Word Count Distribution - Test Set")
axes[1].set_xlabel("Label (Human vs. AI)")
axes[1].set_ylabel("Word Count")

plt.tight_layout()
plt.show()

"""More outliers have been noticed in the training set. It could be due to:
 - the default error of human diversity (styles of writing)
 - the quantity of data in the training set

## **Text Preprocessing (Data Preprocessing)**

**Downloading the stopwords**
"""

nltk.download("stopwords")
stop_words= set(stopwords.words("english"))

stop_words= set(stopwords.words("english"))

def preprocess_text(text):
    text= text.lower()
    text= re.sub(r"[^a-z\s]", "", text)
    words= [word for word in text.split() if word not in stop_words]
    return " ".join(words)

"""**Applying the Preprocessing function we just created to our text**"""

df_train["cleaned_text"]=df_train["text"].apply(preprocess_text)
df_test["cleaned_text"]=df_test["text"].apply(preprocess_text)

"""**Converting text to Numerical features and setting the Target variable (Feature Engineering)**"""

vectorizer= TfidfVectorizer()

X_train= vectorizer.fit_transform(df_train["cleaned_text"])
X_test= vectorizer.transform(df_test["cleaned_text"])

y_train= df_train["label"]
y_test= df_test["label"]

"""# **Initializing and evaluating Machine Learning Algorithms**

## **1. Logistic Regression**

"""

lr= LogisticRegression()
lr.fit(X_train, y_train)

"""**Making predictions on test data**"""

y_pred_lr= lr.predict(X_test)

"""**Evaluating performance with Classification report**"""

print("Classification Report")
print(classification_report(y_test, y_pred_lr))

"""**Confusion Matrix for Logistic Regression**"""

plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, cmap="Blues", fmt="d")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Logistic Regression Confusion Matrix")
plt.show()

"""**Accuracy of Logistic Regression is 96%**

## **2. Random Forest**
"""

rf= RandomForestClassifier()
rf.fit(X_train, y_train)

"""**Making predictions on test data**"""

y_pred_rf= rf.predict(X_test)

"""**Evaluating performance with Classification Report**"""

print("Random Forest Classification Report")
print(classification_report(y_test, y_pred_rf))

"""**Confusion Matrix for Random Forest Classifier**"""

plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, cmap="Reds", fmt="d")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Random Forest Confusion Matrix")
plt.show()

"""**Accuracy of the Random Forest Classifier is 95%**

## **3. Naïve Bayes**
"""

nb= MultinomialNB()
nb.fit(X_train, y_train)

"""**Making predictions on test data**"""

y_pred_nb= nb.predict(X_test)

"""**Evaluating performance with Classification Report**"""

print("Naïve Bayes Classification Report")
print(classification_report(y_test, y_pred_nb))

"""**Confusion Matrix for Naïve Bayes**"""

plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred_nb), annot=True, cmap="Greens", fmt="d")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Naïve Bayes Confusion Matrix")
plt.show()

"""**Accuracy of the Naïve Bayes Classifier is 90%**

## **4. Support Vector Machine (SVM)**
"""

svm= SVC()
svm.fit(X_train, y_train)

"""**Making predictions on test data**"""

y_pred_svm = svm.predict(X_test)

"""**Evaluating performance with Classification Report**"""

print("SVM Classification Report")
print(classification_report(y_test, y_pred_svm))

"""**Confusion Matrix for Support Vector Machine (SVM)**"""

plt.figure(figsize=(5,4))
sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, cmap="Purples", fmt="d")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("SVM Confusion Matrix")
plt.show()

"""### **Accuracy comparison**"""

models=["Naïve Bayes", "Logistic Regression", "Random Forest", "SVM"]
accuracy_scores=[
    accuracy_score(y_test, y_pred_nb),
    accuracy_score(y_test, y_pred_lr),
    accuracy_score(y_test, y_pred_rf),
    accuracy_score(y_test, y_pred_svm)
]

plt.figure(figsize=(8,5))
sns.barplot(x=models, y=accuracy_scores, palette="coolwarm")
plt.ylim(0.5,1)
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy Score")
plt.show()

"""**Visualizing the Classification report results manually**"""

data = {"Model": ["Logistic Regression", "Random Forest", "Naïve Bayes", "SVM"],
       "Precision": [95,97,97,95],
       "Recall": [97,92,82,95],
       "F1-Score": [96,95,89,96],
      "Accuracy": [96,95,90,96]}

metrics= pd.DataFrame(data)
metrics

"""## **Final conclusions on Model Selection**

 - Naïve Bayes and Random Forest have the least accuracy scores in our test. However, very good to go with, Classification problems do better with good accuracy, more so, we want a model that correctly classifies our text.
 - Logistic Regression and Support Vector Machine (SVM) have outperformed on the test set, scoring 96% on the test.
 - However, the Recall rate (Sensitivity) of SVM is lagging behind Logistic in our scenario. It could not predict AI generated content with enough finesse.
 - Moreover, SVMs tend are a lot slower as compared to Logistic Regression, asit is computationally expensive due to solving of quadratic optimization problems.
 - SVM is sensitive to **Noisy Data** and we noticed a lot of outliers in our training set. this might lead to overfitting.
 - We do not have complex relationships within our data, i.e, only 4 columns with minimal correlation and causation between them. Hence Logistic Regression thrived.

We choose to move on with our project by implementing the **Logistic Regression Machine Learning Model**.

## **Making prediction on sample text**
"""

def predict_text(lr,text):
    processed_text= preprocess_text(text)  # Preprocess text
    vectorized_text= vectorizer.transform([processed_text])  # Convert to features
    prediction= lr.predict(vectorized_text)[0]  # Get prediction

    return "AI-Generated" if prediction==1 else "Human-Written"

user_text = input("Enter a text: ")
print("Prediction:", predict_text(lr, user_text))

"""## **Deployment**"""

!pip install streamlit
!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared
!chmod +x cloudflared

import pickle

# Assuming 'vectorizer' and 'best_model' are your trained objects
with open("vectorizer.pkl", "wb") as f:
    pickle.dump(vectorizer, f)

with open("best_model.pkl", "wb") as f:
    pickle.dump(lr, f)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# import pickle
# import nltk
# 
# # Download stopwords
# nltk.download("stopwords")
# 
# # Load the vectorizer and model
# with open("vectorizer.pkl", "rb") as f:
#     vectorizer = pickle.load(f)
# 
# with open("best_model.pkl", "rb") as f:
#     best_model = pickle.load(f)
# 
# # Text preprocessing function
# def preprocess_text(text):
#     text = text.lower()  # Convert to lowercase
#     words = text.split()
#     return " ".join(words)
# 
# # Prediction function
# def predict_with_confidence(text):
#     processed_text = preprocess_text(text)
#     vectorized_text = vectorizer.transform([processed_text])  # Convert to features
#     prediction = best_model.predict(vectorized_text)[0]  # Predict
#     confidence = best_model.predict_proba(vectorized_text)[0].max()  # Get confidence score
# 
#     label = "🟢 AI-Generated" if prediction == 1 else "🔵 Human-Written"
#     return f"{label} (Confidence: {confidence:.2f})"
# 
# # Streamlit UI
# st.title("🤖 AI vs Human Text Classifier")
# st.markdown("Enter a piece of text, and the model will predict whether it's AI-generated or human-written.")
# 
# # Input text box
# text_input = st.text_area("Enter text here:", height=150)
# 
# # Predict button
# if st.button("🔍 Predict"):
#     if text_input.strip():  # Ensure input is not empty
#         result = predict_with_confidence(text_input)
#         st.success(result)
#     else:
#         st.warning("Please enter some text to classify.")
#

!streamlit run app.py & ./cloudflared tunnel --url http://localhost:8501

